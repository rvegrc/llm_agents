{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from tools.memory import save_recall_memory, search_recall_memories\n",
    "from tools.rag import rag_tool\n",
    "from tools.llm import llm_chat_tool\n",
    "from tools.search import search_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2eeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "\n",
    "    vectorstore_long_term_memory.simalarity_search\n",
    "\n",
    "    conv_str = get_buffer_string(state[\"messages\"][-3:]) # get all messages in the conversation or change to 2-3\n",
    "    # conv_str = tokenizer.decode(tokenizer.encode(conv_str)[-2048:]) # tokenize last messages and limit to 2048 tokens\n",
    "    recall_memories = search_recall_memories.invoke(conv_str, config)\n",
    "    return {\n",
    "        \"messages\": recall_memories,\n",
    "    }\n",
    "\n",
    "\n",
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | llm_with_tools\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" + \"\\n\".join(state[\"messages\"]) + \"\\n</recall_memory>\"\n",
    "    )\n",
    "    prediction = bound.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"recall_memories\": recall_str,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # save the response to the long-term memory\n",
    "    prediction.add_document(\n",
    "        Document(\n",
    "            page_content=prediction.content,\n",
    "            metadata={\n",
    "                \"role\": \"assistant\",\n",
    "                \"source\": \"agent_response\",\n",
    "                \"timestamp\": prediction.created_at,\n",
    "                user_id thread id\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"messages\": [prediction],\n",
    "    }\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    \"\"\"Determine whether to use tools or end the conversation based on the last message.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"tools\", \"__end__\"]: The next step in the graph.\n",
    "    \"\"\"\n",
    "    msg = state[\"messages\"][-1]\n",
    "    if msg.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f94f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent():\n",
    "    builder = StateGraph()\n",
    "    builder = StateGraph(State)\n",
    "\n",
    "    builder.add_node(load_memories)\n",
    "    builder.add_node(agent)\n",
    "    builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    builder.add_edge(START, \"load_memories\")\n",
    "    builder.add_edge(\"load_memories\", \"agent\")\n",
    "    builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "    builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    memory = InMemorySaver()\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    \n",
    "\n",
    "    return builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"recall_memory\"\n",
    "\n",
    "vectorstore_recall = rag_tool(\n",
    "    client_qd=client_qd,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bee496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "# from langgraph_agent import build_agent\n",
    "\n",
    "app = FastAPI()\n",
    "agent = build_agent()\n",
    "\n",
    "class UserInput(BaseModel):\n",
    "    message: str\n",
    "    session_id: str  # Optional: for multi-user memory\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(user_input: UserInput):\n",
    "    state = {\"input\": user_input.message, \"session_id\": user_input.session_id}\n",
    "    result = agent.invoke(state)\n",
    "    return {\"response\": result.get(\"input\", \"Sorry, something went wrong.\")}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
