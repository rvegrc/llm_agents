{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d2e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Health Check: 200 {\"status\":\"ok\"}\n"
     ]
    }
   ],
   "source": [
    "import secrets\n",
    "import string\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# set api url\n",
    "# api_url = 'https://example.com/generate' # for dev server\n",
    "api_url = 'http://localhost:8000' # for local testing\n",
    "\n",
    "LLM_API_SERVER_URL = 'http://localhost:11434'  # Ollama server URL\n",
    "\n",
    "API_TOKEN = os.getenv('API_TOKEN')\n",
    "\n",
    "health = requests.get(f'{api_url}/health')\n",
    "\n",
    "print(\"API Health Check:\", health.status_code, health.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c8e6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_ollama_model(\"qwen3:4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e483715",
   "metadata": {},
   "source": [
    "## Test LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cd912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def for test llm\n",
    "def test_llm(prompt: str, user_id: str, thread_id: str) -> None:\n",
    "    api_url = \"http://localhost:8000\"\n",
    "    endpoint = \"/generate\"\n",
    "\n",
    "       \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\"  # add token\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"user_id\": user_id,\n",
    "        \"thread_id\": thread_id\n",
    "    }\n",
    "\n",
    "    response = requests.post(f'{api_url}{endpoint}', json=payload, headers=headers)\n",
    "    print(\"Status code:\", response.status_code)\n",
    "    try:\n",
    "        data = response.json()\n",
    "        print(\"Response:\", data.get(\"generated_text\", data))\n",
    "    except Exception:\n",
    "        print(\"Response Text:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38b6dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Response: </think>\n",
      "\n",
      "Сегодня в Бангладеше в городе Дхака погода: **смешано-ясно**, температура **27,5°C (81,5°F)**, ветер с запада со скоростью **7,8 м/с**, относительная влажность **84%**, давление **1005,0 мб**, без осадков, UV-индекс **0**. \n",
      "\n",
      "Прогноз на сегодня: **patchy rain possible** (возможен дождь). \n",
      "\n",
      "Если нужна точная информация о завтрашней погоде, подождите, пока данные станут доступными.\n"
     ]
    }
   ],
   "source": [
    "user_id = \"user_123\"\n",
    "thread_id = \"thread_123\"\n",
    "\n",
    "test_llm(\"Какая погода сегодня в Бангладеше\", user_id, thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2694c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Response: С预报 на завтра не доступно для Пекина, но по предварительным данным в ноябре 2025 года Пекин может быть холодным и мокрым с низкими температурами и возможным дождем. Для Москвы также нет конкретных данных о завтрашней погоде. Подождите, пока данные станут доступными.\n"
     ]
    }
   ],
   "source": [
    "test_llm(\"А завтра какая погода?\", user_id, thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4caee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "('Погода в Пекине сегодня: ясно, температура 27,2°C (81°F), ветер с запада со '\n",
      " 'скоростью 6,7 м/с, относительная влажность 70%, давление 1003,0 мб, без '\n",
      " 'осадков, UV-индекс 0.  \\n'\n",
      " 'С预报 на завтра не disponible, но по предварительным данным, в ноябре 2025 '\n",
      " 'года Пекин может быть холодным и мокрым с низкими температурами и возможным '\n",
      " 'дождем. Если нужна более точная информация, подождите, пока данные станут '\n",
      " 'доступными.')\n",
      "Response: None\n"
     ]
    }
   ],
   "source": [
    "test_llm(\"БОльше деталей о погоде завтра и на русском языке\", user_id, thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5830ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "('The current weather in Moscow is partly cloudy with a temperature of 20.3°C '\n",
      " '(68.5°F). Wind speed is 5.1 mph from the west, and the pressure is 1015.0 mb '\n",
      " 'with no precipitation. The UV index is 0.1. \\n'\n",
      " '\\n'\n",
      " \"For tomorrow's forecast, specific details are not available yet. However, \"\n",
      " 'the monthly weather forecast suggests generally mild conditions with '\n",
      " \"potential for slight changes. Let me know if you'd like updates once more \"\n",
      " 'data becomes available!')\n",
      "Response: None\n"
     ]
    }
   ],
   "source": [
    "test_llm(\"А в Москве какая погода завтра?\", user_id, thread_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78640a8a",
   "metadata": {},
   "source": [
    "## Function to pull models from ollama server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a50ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_ollama_model(model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Pulls a model to the Ollama server.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model to pull.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The response from the Ollama server if model is successfully pulled.\n",
    "    \"\"\"\n",
    "    url = f\"{LLM_API_SERVER_URL}/api/pull\"\n",
    "    payload = {\n",
    "    \"name\": model_name\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    print(\"Status code:\", response.status_code)\n",
    "    print(\"Response:\", response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
