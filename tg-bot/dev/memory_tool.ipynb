{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4aacbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 23:06:56,965 [INFO] Modules and Environment variables loaded.\n",
      "2025-08-12 23:06:56,966 [INFO] Initializing Qdrant client.\n",
      "2025-08-12 23:06:57,038 [INFO] HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 23:06:57,040 [INFO] Qdrant client initialized.\n",
      "2025-08-12 23:06:57,040 [INFO] Initializing embeddings.\n",
      "2025-08-12 23:06:57,108 [INFO] Loaded embeddings model: nomic-embed-text\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "import logging\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Initialize LangSmith project\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = 'tg-bot'\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "LLM_API_SERVER_URL = os.getenv(\"LLM_API_SERVER_URL\")\n",
    "LLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\")\n",
    "\n",
    "logging.info(\"Modules and Environment variables loaded.\")\n",
    "logging.info(\"Initializing Qdrant client.\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client_qd = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "logging.info(\"Qdrant client initialized.\")\n",
    "\n",
    "# emb_model_name = '/models/multilingual-e5-large-instruct'\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=emb_model_name)\n",
    "\n",
    "logging.info(\"Initializing embeddings.\")\n",
    "\n",
    "emb_model_name = 'nomic-embed-text'\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=LLM_API_SERVER_URL,\n",
    "    model=emb_model_name\n",
    ")\n",
    "\n",
    "logging.info(f\"Loaded embeddings model: {emb_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704c90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 23:07:05,961 [INFO] HTTP Request: GET http://localhost:6333/collections/recall_memories \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 23:07:06,262 [INFO] HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# vectorstore initialization\n",
    "\n",
    "collection_name = 'recall_memories'\n",
    "\n",
    "recall_memories =  QdrantVectorStore(\n",
    "        client=client_qd,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings,\n",
    "        vector_name=\"vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7639946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_user_thread_id(config: RunnableConfig) -> str:\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    thread_id = config[\"configurable\"].get(\"thread_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"User ID needs to be provided to save a memory.\")\n",
    "    if thread_id is None:\n",
    "        raise ValueError(\"Thread ID needs to be provided to save a memory.\")\n",
    "\n",
    "    return user_id, thread_id\n",
    "\n",
    "\n",
    "\n",
    "def save_recall_memories(\n",
    "        memory: str,\n",
    "        config: RunnableConfig       \n",
    "        ) -> str:\n",
    "    \"\"\"Save recall memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id, thread_id = get_user_thread_id(config)\n",
    "    document = Document(\n",
    "        page_content=memory,\n",
    "        metadata={\"user_id\": user_id, 'thread_id': thread_id}\n",
    "    )\n",
    "\n",
    "    print(document)\n",
    "\n",
    "    recall_memories.add_documents([document])\n",
    "    return memory\n",
    "\n",
    "\n",
    "\n",
    "def search_recall_memories(\n",
    "        query: str,\n",
    "        config: RunnableConfig\n",
    ") -> List[str]:\n",
    "    \"\"\"Search for relevant recall memories.\"\"\"\n",
    "\n",
    "    user_id, thread_id = get_user_thread_id(config)\n",
    "    \n",
    "    # Filter by user_id and thread_id\n",
    "    qdrant_filter = models.Filter(\n",
    "        must=[\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.user_id\",\n",
    "                match=models.MatchValue(value=user_id),\n",
    "            ),\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.thread_id\",\n",
    "                match=models.MatchValue(value=thread_id),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    documents = recall_memories.similarity_search(\n",
    "        query,\n",
    "        k=3,\n",
    "        filter=qdrant_filter,  # structured filter required by QdrantVectorStore\n",
    "    )\n",
    "\n",
    "    return [doc.page_content for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d5ff780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from  backend\n",
    "user_id = \"user_123\"\n",
    "thread_id = \"thread_456\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"user_id\": user_id,\n",
    "        \"thread_id\": thread_id\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "926f1ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='погода' metadata={'user_id': 'user_123', 'thread_id': 'thread_456'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'погода'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_recall_memories(\"погода\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67c3768d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['погода']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"погода\"\n",
    "\n",
    "search_recall_memories(query,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5da83518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('user_123', 'thread_456')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_thread_id(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca89dab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vectors shape: 2 x 768\n"
     ]
    }
   ],
   "source": [
    "texts = [\"тест\", \"пример текста\"]\n",
    "vectors = embeddings.embed_documents(texts)\n",
    "print(\"Embedding vectors shape:\", len(vectors), \"x\", len(vectors[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a932b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Record(id='a2767bf6-eb6e-4a6a-a23b-5da19f352575', payload={'page_content': 'погода', 'metadata': {'user_id': 'user_123', 'thread_id': 'thread_456'}}, vector=None, shard_key=None, order_value=None)],\n",
       " None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = 'recall_memories'\n",
    "\n",
    "\n",
    "def search_recall_memories(\n",
    "        query: str,\n",
    "        config: RunnableConfig\n",
    ") -> List[str]:\n",
    "    \"\"\"Search for relevant recall memories.\"\"\"\n",
    "\n",
    "    user_id, thread_id = get_user_thread_id(config)\n",
    "    \n",
    "    # Filter by user_id and thread_id\n",
    "    qdrant_filter = models.Filter(\n",
    "        must=[\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.user_id\",\n",
    "                match=models.MatchValue(value=user_id),\n",
    "            ),\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.thread_id\",\n",
    "                match=models.MatchValue(value=thread_id),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    documents = recall_memories.similarity_search(\n",
    "        query,\n",
    "        k=3,\n",
    "        filter=qdrant_filter,  # structured filter required by QdrantVectorStore\n",
    "    )\n",
    "\n",
    "    return [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae10ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
