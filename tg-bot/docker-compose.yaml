services:
  
  # BACKEND
  ollama:
    # image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    image: ollama/ollama:0.11.4
    container_name: ollama-agents
    volumes:
      - ./backend/data/ollama_data:/root/.ollama
      - ../../models/:/custom/models
    environment:
      - OLLAMA_MODELS=/custom/models
      - OLLAMA_SERVER_HOST=0.0.0.0
    ports:
      - "11434:11434"
    pull_policy: always
    tty: true
    restart: unless-stopped
    networks:
      - llmnet 

  # for production use TLS and generating JSON Web Tokens
  qdrant:
    # use api-key for access
    image: qdrant/qdrant:v1.15
    container_name: qdrant-agents
    volumes:
      - ./backend/vectorstore/qdrant_data:/qdrant/storage:z
    ports:
      - "6333:6333"
      - "6334:6334"
    restart: unless-stopped
    networks:
      - llmnet 
  
  api:
    build: ./backend/api
    container_name: api-agents
    ports:
      - "8000:8000"
    env_file:
     - ./backend/api/.env
    volumes:
      - ./backend/api:/app
      - ../../models:/models
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
    networks:
      - llmnet

  # FRONTEND

  bot:
    build: ./frontend/bot
    container_name: tg-bot
    depends_on:
      - api
    env_file:
      - ./frontend/bot/.env
    volumes:
      - ./frontend/bot:/app
    networks:
      - llmnet

volumes:
  ollama_data: {}
  models: {}
  qdrant_data: {}
  open-webui: {}

networks:
  llmnet:
    driver: bridge

