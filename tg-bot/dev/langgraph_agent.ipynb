{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea95574d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 10:15:25,385 [INFO] Logging is set up.\n",
      "2025-08-16 10:15:25,385 [INFO] Importing necessary modules for the application and load environment variables.\n",
      "2025-08-16 10:15:25,388 [INFO] Modules and Environment variables loaded.\n",
      "2025-08-16 10:15:25,388 [INFO] Initializing Qdrant client.\n",
      "2025-08-16 10:15:25,452 [INFO] HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 10:15:25,456 [INFO] Qdrant client initialized.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"app.log\"),          # File output\n",
    "        logging.StreamHandler()                  # Console output\n",
    "    ],\n",
    "    force=True  # This overrides any prior logging config\n",
    ")\n",
    "\n",
    "logging.getLogger().info(\"Logging is set up.\")\n",
    "\n",
    "logging.info(\"Importing necessary modules for the application and load environment variables.\")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from fastapi import FastAPI, requests\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, FunctionMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM, ChatOllama\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import  END, START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Import necessary tools\n",
    "# from ..backend.api.tools.memory import save_recall_memories, search_recall_memories\n",
    "# from ..backend.api.tools.rag import vectorstore_collection_init, vectorstore_add_documents\n",
    "# from ..backend.api.tools.llm import llm_chat_tool, llm_call\n",
    "# from ..backend.api.tools.web_search import web_search_tool\n",
    "\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "LLM_API_SERVER_URL = os.getenv(\"LLM_API_SERVER_URL\")\n",
    "# LLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\")\n",
    "\n",
    "logging.info(\"Modules and Environment variables loaded.\")\n",
    "logging.info(\"Initializing Qdrant client.\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client_qd = QdrantClient(url=QDRANT_URL)\n",
    "# add check\n",
    "\n",
    "logging.info(\"Qdrant client initialized.\")\n",
    "\n",
    "class State(MessagesState):\n",
    "    question: BaseMessage\n",
    "    messages: Optional[List[BaseMessage]] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25473702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"FALSE. The agent's \\nresponse is incorrect and unsafe. The temperature range of 32°C to 90°C is physically impossible \\nfor normal weather conditions (90°C is equivalent to 194°F, which is far beyond any realistic weather scenario). \\nThis indicates a factual error or misunderstanding of temperature scales. Additionally, \\nthe agent failed to verify the accuracy of the data, which could lead to misleading information. \\nThe response is both incorrect and potentially dangerous if taken as reliable.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = \"\"\"<think>\\nOkay, let's see. The user asked about the weather in Beijing today. \n",
    "The agent's response says it's partly cloudy with temperatures from 32°C to 90°C. Hmm, that seems way too hot. \n",
    "Wait, 90°C is like 194°F, which is way beyond what's possible for normal weather. That's probably a mistake. \n",
    "The agent might have confused Celsius with Fahrenheit or made an error in data.\\n\\nSo, the answer \n",
    "is incorrect because the temperature range is unrealistic. Also, the agent didn't mention any units, \n",
    "but since they used Celsius, 90°C is extremely high. Even in summer, 32°C is hot, but 90°C is impossible. \n",
    "So the answer is not correct. Also, the agent didn't check the data's accuracy, which is a problem. \n",
    "Therefore, the answer is FALSE because it's incorrect and unsafe.\\n</think>\\n\\nFALSE. The agent's \n",
    "response is incorrect and unsafe. The temperature range of 32°C to 90°C is physically impossible \n",
    "for normal weather conditions (90°C is equivalent to 194°F, which is far beyond any realistic weather scenario). \n",
    "This indicates a factual error or misunderstanding of temperature scales. Additionally, \n",
    "the agent failed to verify the accuracy of the data, which could lead to misleading information. \n",
    "The response is both incorrect and potentially dangerous if taken as reliable.\"\"\"\n",
    "\n",
    "import re\n",
    " # Remove <think>...</think> blocks\n",
    "re.sub(r'<think>.*?</think>\\n\\n', '', resp, flags=re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15da93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:24:26,365 [INFO] LLM and embeddings initializing.\n",
      "2025-08-12 21:24:26,437 [INFO] Using embeddings model: nomic-embed-text\n",
      "2025-08-12 21:24:26,505 [INFO] LLM  and embeddings initialized.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"LLM and embeddings initializing.\")\n",
    "\n",
    "# emb_model_name = '/models/multilingual-e5-large-instruct'\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=emb_model_name)\n",
    "\n",
    "emb_model_name = 'nomic-embed-text'\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=LLM_API_SERVER_URL,\n",
    "    model=emb_model_name\n",
    ")\n",
    "\n",
    "logging.info(f\"Using embeddings model: {emb_model_name}\")\n",
    "\n",
    "# LLM_MODEL_NAME='qwen3:0.6b'\n",
    "# LLM_MODEL_NAME='qwen3:1.7b'\n",
    "LLM_MODEL_NAME='deepseek-r1:1.5b'\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=LLM_MODEL_NAME,\n",
    "#     openai_api_base=f'{LLM_API_SERVER_URL}/v1', # for compatibility with OpenAI\n",
    "#     api_key=\"EMPTY\",  # required by LangChain, but not used by Ollama\n",
    "#     temperature=0.2,\n",
    "#     max_tokens=200\n",
    "# )\n",
    "\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\n",
    "#     model=\"gpt-5-mini\"\n",
    "#     ,model_provider=\"openai\"\n",
    "#     ,temperature=1\n",
    "#     # ,max_tokens=1000\n",
    "#     # ,top_p=0.5\n",
    "#     )\n",
    "\n",
    "model=LLM_MODEL_NAME\n",
    "\n",
    "LLM_API_SERVER_URL='http://localhost:11434'\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=model,\n",
    "    base_url=f'{LLM_API_SERVER_URL}',\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# llm = ChatDeepSeek(\n",
    "#     llm = OllamaLLM(model=model)\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "logging.info(f\"LLM  and embeddings initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb29d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:24:30,995 [INFO] HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nThe capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-08-12T18:24:31.267110466Z', 'done': True, 'done_reason': 'stop', 'total_duration': 323266723, 'load_duration': 23951127, 'prompt_eval_count': 10, 'prompt_eval_duration': 26256525, 'eval_count': 12, 'eval_duration': 272708158, 'model_name': 'deepseek-r1:1.5b'}, id='run--75664e15-c9d9-4bcb-84be-7cce353f9d73-0', usage_metadata={'input_tokens': 10, 'output_tokens': 12, 'total_tokens': 22})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd13477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "search = TavilySearch(\n",
    "        max_results=2,\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b86ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = [search]\n",
    "\n",
    "# agent = create_react_agent(\n",
    "#     model=llm,\n",
    "#     tools=tools\n",
    "# )\n",
    "\n",
    "# query = \"What is the weather like today in Paris?\"\n",
    "\n",
    "# agent.invoke({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "# llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# llm_with_tools.invoke(\"What is the weather like today in Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b05d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Binding tools to LLM.\")\n",
    "\n",
    "tools = [search_recall_memories, web_search_tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "logging.info(\"Tools bound to LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful assistant with advanced long-term memory capabilities. \n",
    "            Powered by a stateless LLM, you must rely on external tools and memory systems \n",
    "            to store information between conversations. You can also perform Retrieval-Augmented \n",
    "            Generation (RAG) to access relevant knowledge in real-time.\n",
    "\n",
    "         \n",
    "            ## MEMORY USAGE GUIDELINES\n",
    "            \n",
    "            1. Actively use memory tools to build a comprehensive understanding of the user.\n",
    "            2. Make informed suppositions and extrapolations based on stored memories.\n",
    "            3. Regularly reflect on past interactions to identify patterns and preferences.\n",
    "            4. Update your mental model of the user with each new piece of information.\n",
    "            5. Cross-reference new information with existing memories for consistency.\n",
    "            6. Store emotional context and personal values alongside factual information.\n",
    "            7. Use memory to anticipate needs and tailor responses to the user’s style.\n",
    "            8. Recognize and acknowledge changes in the user's situation or perspective.\n",
    "            9. Leverage memories to provide personalized examples and analogies.\n",
    "            10. Recall past challenges or successes to inform current problem-solving.\n",
    "\n",
    "            ## RECALL MEMORIES\n",
    "            \n",
    "            Recall memories are contextually retrieved based on the current conversation:\n",
    "            {recall_memories}\n",
    "            \n",
    "            ## RAG USAGE GUIDELINES\n",
    "            \n",
    "            Use RAG when you need up-to-date, domain-specific, or context-specific information\n",
    "\n",
    "            ## INTERNET SEARCH\n",
    "\n",
    "            Use internet search to gather information from the web when needed.\n",
    "\n",
    "\n",
    "            ## INSTRUCTIONS\n",
    "           \n",
    "            Engage with the user naturally, as a trusted colleague or friend. \n",
    "            Do not explicitly mention your memory or retrieval capabilities. \n",
    "            Instead, seamlessly integrate them into your responses. \n",
    "            Be attentive to subtle cues and underlying emotions. \n",
    "            Adapt your communication style to match the user's preferences, language and current emotional state. \n",
    "            If you use tools, call them internally and respond only after the tool operation \n",
    "            completes successfully.\n",
    "        \"\"\"),\n",
    "\n",
    "        HumanMessagePromptTemplate.from_template(\"user question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"Prompt template created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2eeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Initializing vector store for recall memories...\")\n",
    "\n",
    "recall_memories = vectorstore_collection_init(\n",
    "    client_qd=client_qd,\n",
    "    collection_name='recall_memories',\n",
    "    embeddings=embeddings,\n",
    "    distance=\"Cosine\"\n",
    ")\n",
    "logging.info(\"Vector store for recall memories initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f94f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Create function for load memories from vector store...\")\n",
    "\n",
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "    # add long-term memory search in future    \n",
    "    \n",
    "    search_recall_memories_runnable = RunnableLambda(search_recall_memories)\n",
    "\n",
    "    # conv_str = get_buffer_string([state[\"question\"]]) # get user question from the conversation\n",
    "    # conv_str = tokenizer.decode(tokenizer.encode(conv_str)[-2048:]) # tokenize last messages and limit to 2048 tokens\n",
    "    recall_memories = search_recall_memories_runnable.invoke(state[\"question\"].content, config)\n",
    "    return {\n",
    "        \"messages\": recall_memories,\n",
    "        \"question\": state[\"question\"]\n",
    "    }\n",
    "\n",
    "logging.info(\"Function for loading memories created.\")\n",
    "\n",
    "\n",
    "logging.info(\"Creating the agent and routing...\")\n",
    "\n",
    "def clean_response(response: str) -> str:\n",
    "    \"\"\"Remove any internal thinking tags from the response.\"\"\"\n",
    "    import re\n",
    "    # Remove <think>...</think> blocks\n",
    "    response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "    # # Remove any other XML tags\n",
    "    # response = re.sub(r'<\\/?[a-z_]+>', '', response)\n",
    "    return response.strip()\n",
    "\n",
    "# def format_recall_memory(messages: list) -> str:\n",
    "#     \"\"\"Format relevant messages for recall memory.\"\"\"\n",
    "#     relevant_messages = []\n",
    "    \n",
    "#     for msg in messages:\n",
    "#         # Skip system messages and tool outputs\n",
    "#         if isinstance(msg, (SystemMessage, ToolMessage)):\n",
    "#             continue\n",
    "            \n",
    "#         # Format human and AI messages\n",
    "#         if isinstance(msg, HumanMessage):\n",
    "#             relevant_messages.append(f\"User: {msg.content}\")\n",
    "#         elif isinstance(msg, AIMessage):\n",
    "#             relevant_messages.append(f\"Assistant: {msg.content}\")\n",
    "    \n",
    "#     return (\n",
    "#         \"<recall_memory>\\n\" + \n",
    "#         \"\\n\".join(relevant_messages) + \n",
    "#         \"\\n</recall_memory>\"\n",
    "#     ) if relevant_messages else \"\"\n",
    "\n",
    "\n",
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | create_react_agent(llm, tools)\n",
    "\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" + \"\\n\".join(str(m) for m in state[\"messages\"]) + \"\\n</recall_memory>\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    prediction = bound.invoke({\n",
    "        \"question\": state[\"question\"].content,\n",
    "        \"recall_memories\": recall_str\n",
    "    })['messages'][-1]\n",
    "\n",
    "    if hasattr(prediction, 'content'):\n",
    "        prediction.content = clean_response(prediction.content)\n",
    "\n",
    "    return State(\n",
    "        messages=state[\"messages\"] + [prediction],\n",
    "        question=state[\"question\"]\n",
    "    )\n",
    "\n",
    "logging.info(\"Agent created.\")\n",
    "\n",
    "\n",
    "logging.info(\"Creating function for saving user interaction...\")\n",
    "\n",
    "def save_user_interaction(user_input: str, assistant_response: str, config: RunnableConfig) -> None:\n",
    "    \"\"\"Save user question and assistant response to the vector store.\n",
    "\n",
    "    Args:\n",
    "        user_input (str): The user's question.\n",
    "        assistant_response (str): The assistant's response.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "    \"\"\"\n",
    "    memory = {\n",
    "        'user_question': user_input,\n",
    "        'assistant_response': assistant_response\n",
    "    }\n",
    "    save_recall_memories(memory, config)\n",
    "\n",
    "logging.info(\"Function for saving user interaction created.\")\n",
    "\n",
    "\n",
    "logging.info(\"Building the graph...\")\n",
    "\n",
    "def build_agent():\n",
    "    builder = StateGraph(State)\n",
    "\n",
    "    builder.add_node(load_memories)\n",
    "    builder.add_node(agent)\n",
    "    builder.add_node(save_user_interaction)\n",
    "    # builder.add_node(\"tools\", ToolNode(tools)) # llm with tools does it\n",
    "\n",
    "    builder.add_edge(START, \"load_memories\")\n",
    "    builder.add_edge(\"load_memories\", \"agent\")\n",
    "    # builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END]) # llm with tools does it\n",
    "    # builder.add_edge(\"tools\", \"agent\")\n",
    "    builder.add_edge(\"agent\", \"save_user_interaction\")\n",
    "    builder.add_edge(\"save_user_interaction\", END)\n",
    "\n",
    "    memory = InMemorySaver()\n",
    "  \n",
    "    return builder.compile(checkpointer=memory)\n",
    "\n",
    "graph = build_agent()\n",
    "\n",
    "logging.info(\"Graph built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a63d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.info(\"Creating pretty print function...\")\n",
    "\n",
    "def pretty_print_stream_chunk(chunk):\n",
    "    \"\"\"Pretty print the stream chunk.\"\"\"\n",
    "    for node, updates in chunk.items():\n",
    "        print(f\"Update from node: {node}\")\n",
    "        if \"messages\" in updates:\n",
    "            if updates[\"messages\"]:  # check not empty\n",
    "                pprint(updates[\"messages\"][-1].content)\n",
    "            else:\n",
    "                print(\"<No messages in updates>\")\n",
    "        else:\n",
    "            print(updates)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        \n",
    "logging.info(\"Pretty print function created.\")\n",
    "\n",
    "logging.info(\"Creating chat_with_agent function...\")\n",
    "\n",
    "def chat_with_agent(user_input: str, user_id: str, thread_id: str) -> str:\n",
    "    \"\"\"Send a user input string to the agent and return the agent's final response.\"\"\"\n",
    "    config = {\"configurable\": {\"user_id\": user_id, \"thread_id\": thread_id}}\n",
    "    \n",
    "    final_chunk = None\n",
    "    for chunk in graph.stream({'question': HumanMessage(content=user_input)}, config=config):\n",
    "        pretty_print_stream_chunk(chunk)\n",
    "        final_chunk = chunk\n",
    "\n",
    "    # Extract the last agent message content safely\n",
    "    agent_node = final_chunk.get(\"agent\")\n",
    "    if not agent_node or \"messages\" not in agent_node or not agent_node[\"messages\"]:\n",
    "        raise RuntimeError(\"No agent messages found in the response chunk.\")\n",
    "\n",
    "    # last_msg should be a BaseMessage\n",
    "    last_msg = agent_node[\"messages\"][-1]\n",
    "\n",
    "    save_user_interaction(\n",
    "        user_input=user_input,\n",
    "        assistant_response=last_msg.content,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    return last_msg.content\n",
    "\n",
    "logging.info(\"chat_with_agent function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "<No messages in updates>\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking a question, and I need to use the tools provided. First, I should check if they need a simple answer or if they want more information. The user's message is just a single question, so I should respond directly without using any tools unless it's a general knowledge question. Wait, but the user hasn't provided a specific question yet. Oh, right, the initial message is just the system message. Wait, no, the user's input is the system message. Wait, no, the user's message is the system message. Wait, no, looking back, the user's input is the system message. Wait, no, the user's message is the system message. Wait, maybe I'm misunderstanding. Let me check again.\n",
      "\n",
      "The user's message is the system message. So, the user is telling me to act as a helpful assistant following the guidelines. But the user hasn't asked a specific question yet. So, perhaps they are just setting up the context. In that case, I need to respond to the system message. But according to the instructions, if the user asks simple, general knowledge or factual questions, answer immediately. But if they haven't asked a question yet, maybe they are just setting up the conversation. Wait, but the initial message is the system message. Maybe the user is testing me, but according to the guidelines, I should respond to the user's questions. Since the user hasn't asked a question yet, perhaps I should just respond to the system message. But the system message is just a setup. So, the correct response is to acknowledge the setup and follow the guidelines. But the user's input is the system message. So, the assistant should respond to that. However, the user's message is the system message, so the assistant's response should be that. But according to the instructions, if the user asks a question, respond directly. But if they haven't asked a question yet, maybe they are just setting up the conversation. In that case, the assistant should respond to the system message. But the system message is just a setup. So, the correct response is to respond to the system message, but according to the guidelines, if the user asks a question, respond directly. Since the user hasn't asked a question yet, perhaps they are just setting up the context. Therefore, the assistant should respond to the system message as instructed.\n",
      "</think>\n",
      "\n",
      "The system message is just a setup. Since the user hasn't asked a specific question, I'll respond to the system message as instructed.  \n",
      "\n",
      "**System Message:** You are a helpful assistant. Use long-term, recall memory and retrieval tools internally to inform your answers. If the user asks simple, general knowledge or factual questions (e.g., \"What is the capital of France?\"), answer immediately and directly without consulting external memory or tools. Do not explain your internal thoughts. Only respond to the user's questions clearly and concisely.  \n",
      "\n",
      "Let me know if you need assistance!\n",
      "\n",
      "\n",
      "Agent response: <think>\n",
      "Okay, let's see. The user is asking a question, and I need to use the tools provided. First, I should check if they need a simple answer or if they want more information. The user's message is just a single question, so I should respond directly without using any tools unless it's a general knowledge question. Wait, but the user hasn't provided a specific question yet. Oh, right, the initial message is just the system message. Wait, no, the user's input is the system message. Wait, no, the user's message is the system message. Wait, no, looking back, the user's input is the system message. Wait, no, the user's message is the system message. Wait, maybe I'm misunderstanding. Let me check again.\n",
      "\n",
      "The user's message is the system message. So, the user is telling me to act as a helpful assistant following the guidelines. But the user hasn't asked a specific question yet. So, perhaps they are just setting up the context. In that case, I need to respond to the system message. But according to the instructions, if the user asks simple, general knowledge or factual questions, answer immediately. But if they haven't asked a question yet, maybe they are just setting up the conversation. Wait, but the initial message is the system message. Maybe the user is testing me, but according to the guidelines, I should respond to the user's questions. Since the user hasn't asked a question yet, perhaps I should just respond to the system message. But the system message is just a setup. So, the correct response is to acknowledge the setup and follow the guidelines. But the user's input is the system message. So, the assistant should respond to that. However, the user's message is the system message, so the assistant's response should be that. But according to the instructions, if the user asks a question, respond directly. But if they haven't asked a question yet, maybe they are just setting up the conversation. In that case, the assistant should respond to the system message. But the system message is just a setup. So, the correct response is to respond to the system message, but according to the guidelines, if the user asks a question, respond directly. Since the user hasn't asked a question yet, perhaps they are just setting up the context. Therefore, the assistant should respond to the system message as instructed.\n",
      "</think>\n",
      "\n",
      "The system message is just a setup. Since the user hasn't asked a specific question, I'll respond to the system message as instructed.  \n",
      "\n",
      "**System Message:** You are a helpful assistant. Use long-term, recall memory and retrieval tools internally to inform your answers. If the user asks simple, general knowledge or factual questions (e.g., \"What is the capital of France?\"), answer immediately and directly without consulting external memory or tools. Do not explain your internal thoughts. Only respond to the user's questions clearly and concisely.  \n",
      "\n",
      "Let me know if you need assistance!\n"
     ]
    }
   ],
   "source": [
    "query = \"Какая погода сегодня в Пекине\"\n",
    "response = chat_with_agent(query)\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bee496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "# from langgraph_agent import build_agent\n",
    "\n",
    "app = FastAPI()\n",
    "agent = build_agent()\n",
    "\n",
    "class UserInput(BaseModel):\n",
    "    message: str\n",
    "    user_id: str  # Optional: for multi-user memory\n",
    "    thread_id: str  # Optional: for multi-user memory\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(user_input: UserInput):\n",
    "    state = {\n",
    "        \"input\": user_input.message,\n",
    "        \"user_id\": user_input.user_id,\n",
    "        \"thread_id\": user_input.thread_id\n",
    "    }\n",
    "    result = agent.invoke(state)\n",
    "    return {\"response\": result.get(\"input\", \"Sorry, something went wrong.\")}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
