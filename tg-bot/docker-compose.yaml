services:
  ollama:
    container_name: ollama-agents
    volumes:
      - ./backend/data/ollama_data:/root/.ollama
      - ../../models/:/custom/models
    environment:
      - OLLAMA_MODELS=/custom/models
      - OLLAMA_SERVER_HOST=0.0.0.0
    ports:
      - "11434:11434"
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    # image: ollama/ollama:0.5.8
    networks:
      - llmnet 

  # for production use TLS and generating JSON Web Tokens
  qdrant:
    # use api-key for access
    image: qdrant/qdrant:v1.15
    container_name: qdrant-agents
    volumes:
      - ./backend/vectorstore/qdrant_data:/qdrant/storage:z
    ports:
      - "6333:6333"
      - "6334:6334"
    restart: unless-stopped
    networks:
      - llmnet 

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui-agents
    volumes:
      - ./frontend/data/open-webui:/app/backend/data
    depends_on:
      - ollama
      - qdrant
    ports:
      - ${OPEN_WEBUI_PORT-3001}:8080
    env_file:
      - .env
      # - OLLAMA_BASE_URL=
      # - WEBUI_SECRET_KEY=
      # - VECTOR_DB=qdrant
      # - QDRANT_URI=http://qdrant:6333/
      # - QDRANT_API_KEY=your_api_key_here   # only if you use api key auth
    environment:
      - QDRANT_URI=${QDRANT_URL}
      - VECTOR_DB=${VECTOR_DB}
    restart: unless-stopped
    networks:
      - llmnet 

  # embeddings:
  #   image: ghcr.io/huggingface/text-embeddings-inference:latest
  #   ports:
  #     - "28080:80"
  #   volumes:
  #     - ../../models:/models
  #   environment:
  #     - MODEL_ID=intfloat/multilingual-e5-large-instruct  # Official Hugging Face ID
  #   networks:
  #     - llmnet

  api:
    build: ./api
    container_name: agents-api
    ports:
      - "8000:8000"
    env_file:
     - ./api/.env
    volumes:
      - ./api:/app
      - ../../models:/models
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
    networks:
      - llmnet

  bot:
    build: ./bot
    container_name: tg-bot
    depends_on:
      - api
    env_file:
      - ./bot/.env
    volumes:
      - ./bot:/app
    networks:
      - llmnet



volumes:  
  clickhouse_data: {}
  qdrant_data: {}

networks:
  llmnet:
    driver: bridge

