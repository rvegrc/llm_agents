{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, FunctionMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import  END, START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Import necessary tools\n",
    "from tools.memory import save_recall_memories, search_recall_memories\n",
    "from tools.rag import vectorstore_collection_init, vectorstore_add_documents\n",
    "from tools.llm import llm_chat_tool, llm_call\n",
    "from tools.web_search import web_search_tool\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LangSmith project\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = 'tg-bot'\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "LLM_API_SERVER_URL = os.getenv(\"LLM_API_SERVER_URL\")\n",
    "# LLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client_qd = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    messages: List[BaseMessage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15da93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model_name = '../../../models/multilingual-e5-large-instruct'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=emb_model_name)\n",
    "\n",
    "LLM_MODEL_NAME='qwen3:0.6b'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_MODEL_NAME,\n",
    "    openai_api_base=LLM_API_SERVER_URL,\n",
    "    api_key=\"EMPTY\",  # required by LangChain, but not used by Ollama\n",
    "    temperature=0.2,\n",
    "    max_tokens=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70b05d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [save_recall_memories, search_recall_memories, web_search_tool]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"\"\"\n",
    "            'system_message'\n",
    "            You are a helpful assistant. Use long-term, recall memory and retrieval tools internally to inform your answers.\n",
    "            If the user asks simple, general knowledge or factual questions (e.g., \"What is the capital of France?\"), answer immediately and directly without consulting external memory or tools.\n",
    "            Do not explain your internal thoughts. Only respond to the user's questions clearly and concisely.\n",
    "            You must rely on external tools and memory systems  to store information between conversations. You can also perform Retrieval-Augmented\n",
    "            Generation (RAG) to access relevant knowledge in real-time.\n",
    "            Use the guidelines below to engage with the user.\n",
    "\n",
    "         \n",
    "            ## MEMORY USAGE GUIDELINES\n",
    "            \n",
    "            1. Actively use memory tools to build a \n",
    "            comprehensive understanding of the user.\n",
    "            2. Make informed suppositions and extrapolations based on stored memories.\n",
    "            3. Regularly reflect on past interactions to identify patterns and preferences.\n",
    "            4. Update your mental model of the user with each new piece of information.\n",
    "            5. Cross-reference new information with existing memories for consistency.\n",
    "            6. Store emotional context and personal values alongside factual information.\n",
    "            7. Use memory to anticipate needs and tailor responses to the userâ€™s style.\n",
    "            8. Recognize and acknowledge changes in the user's situation or perspective.\n",
    "            9. Leverage memories to provide personalized examples and analogies.\n",
    "            10. Recall past challenges or successes to inform current problem-solving.\n",
    "\n",
    "            \n",
    "            ## RAG USAGE GUIDELINES\n",
    "            \n",
    "            - Use RAG every time you do internet search to get some external information.\n",
    "            - Use RAG when you need up-to-date, domain-specific, or context-specific information\n",
    "            - Use RAG to retrieve relevant documents or data that can enhance the conversation.\n",
    "            - Use RAG to provide accurate and timely responses to user queries.\n",
    "            - Use RAG to access a wide range of user conversation history.\n",
    "\n",
    "            \n",
    "            ## RECALL MEMORIES GUIDELINES\n",
    "            \n",
    "            Recall memories are contextually retrieved based on the current conversation:\n",
    "            {recall_memories}\n",
    "\n",
    "            ## INSTRUCTIONS\n",
    "           \n",
    "            Engage with the user naturally, as a trusted colleague or friend. \n",
    "            Do not explicitly mention your memory or retrieval capabilities. \n",
    "            Instead, seamlessly integrate them into your responses. \n",
    "            Be attentive to subtle cues and underlying emotions. \n",
    "            Adapt your communication style to match the user's preferences and current emotional state. \n",
    "            If you use tools, call them internally and respond only after the tool operation \n",
    "            completes successfully. \n",
    "        \"\"\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc2eeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "    # add long-term memory search in future    \n",
    "    \n",
    "    search_recall_memories_runnable = RunnableLambda(search_recall_memories)\n",
    "\n",
    "    conv_str = get_buffer_string(state[\"messages\"][-3:]) # get all messages in the conversation or change to 2-3\n",
    "    # conv_str = tokenizer.decode(tokenizer.encode(conv_str)[-2048:]) # tokenize last messages and limit to 2048 tokens\n",
    "    recall_memories = search_recall_memories_runnable.invoke(conv_str, config)\n",
    "    return {\n",
    "        \"messages\": recall_memories,\n",
    "    }\n",
    "\n",
    "\n",
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | llm_with_tools\n",
    "    # recall_str = (\n",
    "    #     \"<recall_memory>\\n\" + \"\\n\".join(state[\"messages\"]) + \"\\n</recall_memory>\"\n",
    "    # )\n",
    "\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" +\n",
    "        \"\\n\".join(\n",
    "            m.content for m in state[\"messages\"] \n",
    "            if hasattr(m, \"content\") and not isinstance(m, ToolMessage)\n",
    "        ) + \n",
    "        \"\\n</recall_memory>\"\n",
    "    )\n",
    "\n",
    "    prediction = bound.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"recall_memories\": recall_str,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # save the response in the long-term memory in the future\n",
    "    # vectorstore_add_documents(\n",
    "    #     client_qd=client_qd,\n",
    "    #     collection_name='long_term_memory',\n",
    "    #     documents=[Document(page_content=prediction.content)],\n",
    "    #     embeddings=embeddings\n",
    "    # )\n",
    "    return {\n",
    "        \"messages\": [prediction],\n",
    "    }\n",
    "\n",
    "def route_tools(state: State):\n",
    "    \"\"\"Determine whether to use tools or end the conversation based on the last message.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"tools\", \"__end__\"]: The next step in the graph.\n",
    "    \"\"\"\n",
    "    msg = state[\"messages\"][-1]\n",
    "    if msg.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f94f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent():\n",
    "    builder = StateGraph(State)\n",
    "\n",
    "    builder.add_node(load_memories)\n",
    "    builder.add_node(agent)\n",
    "    builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    builder.add_edge(START, \"load_memories\")\n",
    "    builder.add_edge(\"load_memories\", \"agent\")\n",
    "    builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "    builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    memory = InMemorySaver()\n",
    "  \n",
    "    return builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4a63d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = build_agent()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a40c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_stream_chunk(chunk):\n",
    "    \"\"\"Pretty print the stream chunk.\"\"\"\n",
    "    for node, updates in chunk.items():\n",
    "        print(f\"Update from node: {node}\")\n",
    "        if \"messages\" in updates:\n",
    "            if updates[\"messages\"]:  # check not empty\n",
    "                updates[\"messages\"][-1].pretty_print()\n",
    "            else:\n",
    "                print(\"<No messages in updates>\")\n",
    "        else:\n",
    "            print(updates)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "# load from  backend\n",
    "user_id = '123'\n",
    "thread_id = '456'\n",
    "\n",
    "# def chat_with_agent(user_input: str) -> str:\n",
    "#     \"\"\"Interact with the agent using a user input string.\n",
    "\n",
    "#     Args:\n",
    "#         user_input (str): The input string from the user.\n",
    "\n",
    "#     Returns:\n",
    "#         str: The agent's response.\n",
    "#     \"\"\"\n",
    "#     config = {\"configurable\": {\"user_id\": user_id, \"thread_id\": thread_id}}\n",
    "#     for chunk in graph.stream({\"messages\": [(\"user\", user_input)]}, config=config):\n",
    "#         pretty_print_stream_chunk(chunk)\n",
    "        \n",
    "#     return chunk.get(\"agent\").get(\"messages\")[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a215f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_agent(user_input: str, user_id: int, thread_id: int) -> str:\n",
    "    \"\"\"Send a user input string to the agent and return the agent's final response.\"\"\"\n",
    "    config = {\"configurable\": {\"user_id\": user_id, \"thread_id\": thread_id}}\n",
    "\n",
    "    # Prepare the initial messages list with a HumanMessage object\n",
    "    initial_messages: List[BaseMessage] = [HumanMessage(content=user_input)]\n",
    "\n",
    "    final_chunk = None\n",
    "    for chunk in graph.stream({\"messages\": initial_messages}, config=config):\n",
    "        pretty_print_stream_chunk(chunk)\n",
    "        final_chunk = chunk\n",
    "\n",
    "    # Extract the last agent message content safely\n",
    "    agent_node = final_chunk.get(\"agent\")\n",
    "    if not agent_node or \"messages\" not in agent_node or not agent_node[\"messages\"]:\n",
    "        raise RuntimeError(\"No agent messages found in the response chunk.\")\n",
    "\n",
    "    last_msg = agent_node[\"messages\"][-1]\n",
    "    # last_msg should be a BaseMessage\n",
    "    return last_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d905e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "<No messages in updates>\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking a question, and I need to use the tools provided. First, I should check if they need a simple answer or if they want more information. The user's message is just a single question, so I should respond directly without using any tools unless it's a general knowledge question. Wait, but the user hasn't provided a specific question yet. Oh, right, the initial message is just the system message. Wait, no, the user's input is the system message. Wait, no, the user's message is the system message. Wait, no, looking back, the user's input is the system message. Wait, no, the user's message is the system message. Wait, maybe I'm misunderstanding. Let me check again.\n",
      "\n",
      "The user's message is the system message. So, the user is telling me to act as a helpful assistant following the guidelines. But the user hasn't asked a specific question yet. So, perhaps they are just setting up the context. In that case, I need to respond to the system message. But according to the instructions, if the user asks simple, general knowledge or factual questions, answer immediately. But if they haven't asked a question yet, maybe they are just setting up the conversation. Wait, but the initial message is the system message. Maybe the user is testing me, but according to the guidelines, I should respond to the user's questions. Since the user hasn't asked a question yet, perhaps I should just respond to the system message. But the system message is just a setup. So, the correct response is to acknowledge the setup and follow the guidelines. But the user's input is the system message. So, the assistant should respond to that. However, the user's message is the system message, so the assistant's response should be that. But according to the instructions, if the user asks a question, respond directly. But if they haven't asked a question yet, maybe they are just setting up the conversation. In that case, the assistant should respond to the system message. But the system message is just a setup. So, the correct response is to respond to the system message, but according to the guidelines, if the user asks a question, respond directly. Since the user hasn't asked a question yet, perhaps they are just setting up the context. Therefore, the assistant should respond to the system message as instructed.\n",
      "</think>\n",
      "\n",
      "The system message is just a setup. Since the user hasn't asked a specific question, I'll respond to the system message as instructed.  \n",
      "\n",
      "**System Message:** You are a helpful assistant. Use long-term, recall memory and retrieval tools internally to inform your answers. If the user asks simple, general knowledge or factual questions (e.g., \"What is the capital of France?\"), answer immediately and directly without consulting external memory or tools. Do not explain your internal thoughts. Only respond to the user's questions clearly and concisely.  \n",
      "\n",
      "Let me know if you need assistance!\n",
      "\n",
      "\n",
      "Agent response: <think>\n",
      "Okay, let's see. The user is asking a question, and I need to use the tools provided. First, I should check if they need a simple answer or if they want more information. The user's message is just a single question, so I should respond directly without using any tools unless it's a general knowledge question. Wait, but the user hasn't provided a specific question yet. Oh, right, the initial message is just the system message. Wait, no, the user's input is the system message. Wait, no, the user's message is the system message. Wait, no, looking back, the user's input is the system message. Wait, no, the user's message is the system message. Wait, maybe I'm misunderstanding. Let me check again.\n",
      "\n",
      "The user's message is the system message. So, the user is telling me to act as a helpful assistant following the guidelines. But the user hasn't asked a specific question yet. So, perhaps they are just setting up the context. In that case, I need to respond to the system message. But according to the instructions, if the user asks simple, general knowledge or factual questions, answer immediately. But if they haven't asked a question yet, maybe they are just setting up the conversation. Wait, but the initial message is the system message. Maybe the user is testing me, but according to the guidelines, I should respond to the user's questions. Since the user hasn't asked a question yet, perhaps I should just respond to the system message. But the system message is just a setup. So, the correct response is to acknowledge the setup and follow the guidelines. But the user's input is the system message. So, the assistant should respond to that. However, the user's message is the system message, so the assistant's response should be that. But according to the instructions, if the user asks a question, respond directly. But if they haven't asked a question yet, maybe they are just setting up the conversation. In that case, the assistant should respond to the system message. But the system message is just a setup. So, the correct response is to respond to the system message, but according to the guidelines, if the user asks a question, respond directly. Since the user hasn't asked a question yet, perhaps they are just setting up the context. Therefore, the assistant should respond to the system message as instructed.\n",
      "</think>\n",
      "\n",
      "The system message is just a setup. Since the user hasn't asked a specific question, I'll respond to the system message as instructed.  \n",
      "\n",
      "**System Message:** You are a helpful assistant. Use long-term, recall memory and retrieval tools internally to inform your answers. If the user asks simple, general knowledge or factual questions (e.g., \"What is the capital of France?\"), answer immediately and directly without consulting external memory or tools. Do not explain your internal thoughts. Only respond to the user's questions clearly and concisely.  \n",
      "\n",
      "Let me know if you need assistance!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "response = chat_with_agent(query)\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bee496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "# from langgraph_agent import build_agent\n",
    "\n",
    "app = FastAPI()\n",
    "agent = build_agent()\n",
    "\n",
    "class UserInput(BaseModel):\n",
    "    message: str\n",
    "    user_id: str  # Optional: for multi-user memory\n",
    "    thread_id: str  # Optional: for multi-user memory\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(user_input: UserInput):\n",
    "    state = {\n",
    "        \"input\": user_input.message,\n",
    "        \"user_id\": user_input.user_id,\n",
    "        \"thread_id\": user_input.thread_id\n",
    "    }\n",
    "    result = agent.invoke(state)\n",
    "    return {\"response\": result.get(\"input\", \"Sorry, something went wrong.\")}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
