{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4aacbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 22:31:00,065 [INFO] Modules and Environment variables loaded.\n",
      "2025-08-12 22:31:00,065 [INFO] Initializing Qdrant client.\n",
      "2025-08-12 22:31:00,139 [INFO] HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 22:31:00,141 [INFO] Qdrant client initialized.\n",
      "2025-08-12 22:31:00,141 [INFO] Initializing embeddings.\n",
      "2025-08-12 22:31:00,209 [INFO] Loaded embeddings model: nomic-embed-text\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "import logging\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Initialize LangSmith project\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = 'tg-bot'\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "LLM_API_SERVER_URL = os.getenv(\"LLM_API_SERVER_URL\")\n",
    "LLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\")\n",
    "\n",
    "logging.info(\"Modules and Environment variables loaded.\")\n",
    "logging.info(\"Initializing Qdrant client.\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client_qd = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "logging.info(\"Qdrant client initialized.\")\n",
    "\n",
    "# emb_model_name = '/models/multilingual-e5-large-instruct'\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=emb_model_name)\n",
    "\n",
    "logging.info(\"Initializing embeddings.\")\n",
    "\n",
    "emb_model_name = 'nomic-embed-text'\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=LLM_API_SERVER_URL,\n",
    "    model=emb_model_name\n",
    ")\n",
    "\n",
    "logging.info(f\"Loaded embeddings model: {emb_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704c90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 22:31:10,424 [INFO] HTTP Request: GET http://localhost:6333/collections/recall_memories \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 22:31:10,428 [INFO] HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model \"nomic-embed-text\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# vectorstore initialization\u001b[39;00m\n\u001b[32m      3\u001b[39m collection_name = \u001b[33m'\u001b[39m\u001b[33mrecall_memories\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m recall_memories =  QdrantVectorStore(\n\u001b[32m      6\u001b[39m         client=client_qd,\n\u001b[32m      7\u001b[39m         collection_name=collection_name,\n\u001b[32m      8\u001b[39m         embedding=embeddings,\n\u001b[32m      9\u001b[39m         vector_name=\u001b[33m\"\u001b[39m\u001b[33mvector\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_agents/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:213\u001b[39m, in \u001b[36mQdrantVectorStore.__init__\u001b[39m\u001b[34m(self, client, collection_name, embedding, retrieval_mode, vector_name, content_payload_key, metadata_payload_key, distance, sparse_embedding, sparse_vector_name, validate_embeddings, validate_collection_config)\u001b[39m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_embeddings(retrieval_mode, embedding, sparse_embedding)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_collection_config:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_collection_config(\n\u001b[32m    214\u001b[39m         client,\n\u001b[32m    215\u001b[39m         collection_name,\n\u001b[32m    216\u001b[39m         retrieval_mode,\n\u001b[32m    217\u001b[39m         vector_name,\n\u001b[32m    218\u001b[39m         sparse_vector_name,\n\u001b[32m    219\u001b[39m         distance,\n\u001b[32m    220\u001b[39m         embedding,\n\u001b[32m    221\u001b[39m     )\n\u001b[32m    223\u001b[39m \u001b[38;5;28mself\u001b[39m._client = client\n\u001b[32m    224\u001b[39m \u001b[38;5;28mself\u001b[39m.collection_name = collection_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_agents/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:1050\u001b[39m, in \u001b[36mQdrantVectorStore._validate_collection_config\u001b[39m\u001b[34m(cls, client, collection_name, retrieval_mode, vector_name, sparse_vector_name, distance, embedding)\u001b[39m\n\u001b[32m   1038\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_collection_config\u001b[39m(\n\u001b[32m   1040\u001b[39m     \u001b[38;5;28mcls\u001b[39m: Type[QdrantVectorStore],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1047\u001b[39m     embedding: Optional[Embeddings],\n\u001b[32m   1048\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m retrieval_mode == RetrievalMode.DENSE:\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m         \u001b[38;5;28mcls\u001b[39m._validate_collection_for_dense(\n\u001b[32m   1051\u001b[39m             client, collection_name, vector_name, distance, embedding\n\u001b[32m   1052\u001b[39m         )\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m retrieval_mode == RetrievalMode.SPARSE:\n\u001b[32m   1055\u001b[39m         \u001b[38;5;28mcls\u001b[39m._validate_collection_for_sparse(\n\u001b[32m   1056\u001b[39m             client, collection_name, sparse_vector_name\n\u001b[32m   1057\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_agents/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:1109\u001b[39m, in \u001b[36mQdrantVectorStore._validate_collection_for_dense\u001b[39m\u001b[34m(cls, client, collection_name, vector_name, distance, dense_embeddings)\u001b[39m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m vector_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mVectorParams is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dense_embeddings, Embeddings):\n\u001b[32m-> \u001b[39m\u001b[32m1109\u001b[39m     vector_size = \u001b[38;5;28mlen\u001b[39m(dense_embeddings.embed_documents([\u001b[33m\"\u001b[39m\u001b[33mdummy_text\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[32m0\u001b[39m])\n\u001b[32m   1110\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dense_embeddings, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m   1111\u001b[39m     vector_size = \u001b[38;5;28mlen\u001b[39m(dense_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_agents/lib/python3.12/site-packages/langchain_ollama/embeddings.py:284\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    279\u001b[39m     msg = (\n\u001b[32m    280\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOllama client is not initialized. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure Ollama is running and the model is loaded.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m     )\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.embed(\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, texts, options=\u001b[38;5;28mself\u001b[39m._default_params, keep_alive=\u001b[38;5;28mself\u001b[39m.keep_alive\n\u001b[32m    286\u001b[39m )[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_agents/lib/python3.12/site-packages/ollama/_client.py:367\u001b[39m, in \u001b[36mClient.embed\u001b[39m\u001b[34m(self, model, input, truncate, options, keep_alive)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\n\u001b[32m    360\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    361\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    366\u001b[39m ) -> EmbedResponse:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    368\u001b[39m     EmbedResponse,\n\u001b[32m    369\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    370\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/embed\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    371\u001b[39m     json=EmbedRequest(\n\u001b[32m    372\u001b[39m       model=model,\n\u001b[32m    373\u001b[39m       \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    374\u001b[39m       truncate=truncate,\n\u001b[32m    375\u001b[39m       options=options,\n\u001b[32m    376\u001b[39m       keep_alive=keep_alive,\n\u001b[32m    377\u001b[39m     ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    378\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_agents/lib/python3.12/site-packages/ollama/_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28mself\u001b[39m._request_raw(*args, **kwargs).json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_agents/lib/python3.12/site-packages/ollama/_client.py:124\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    126\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model \"nomic-embed-text\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "# vectorstore initialization\n",
    "\n",
    "collection_name = 'recall_memories'\n",
    "\n",
    "recall_memories =  QdrantVectorStore(\n",
    "        client=client_qd,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings,\n",
    "        vector_name=\"vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7639946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_user_thread_id(config: RunnableConfig) -> str:\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    thread_id = config[\"configurable\"].get(\"thread_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"User ID needs to be provided to save a memory.\")\n",
    "    if thread_id is None:\n",
    "        raise ValueError(\"Thread ID needs to be provided to save a memory.\")\n",
    "\n",
    "    return user_id, thread_id\n",
    "\n",
    "\n",
    "\n",
    "def save_recall_memories(\n",
    "        memory: str,\n",
    "        config: RunnableConfig       \n",
    "        ) -> str:\n",
    "    \"\"\"Save recall memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id, thread_id = get_user_thread_id(config)\n",
    "    document = Document(\n",
    "        page_content=memory,\n",
    "        metadata={\"user_id\": user_id, 'thread_id': thread_id}\n",
    "    )\n",
    "\n",
    "    print(document)\n",
    "\n",
    "    recall_memories.add_documents([document])\n",
    "    return memory\n",
    "\n",
    "\n",
    "\n",
    "def search_recall_memories(\n",
    "        query: str,\n",
    "        config: RunnableConfig\n",
    ") -> List[str]:\n",
    "    \"\"\"Search for relevant recall memories.\"\"\"\n",
    "\n",
    "    user_id, thread_id = get_user_thread_id(config)\n",
    "    \n",
    "    # Filter by user_id and thread_id\n",
    "    qdrant_filter = models.Filter(\n",
    "        must=[\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.user_id\",\n",
    "                match=models.MatchValue(value=user_id),\n",
    "            ),\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.thread_id\",\n",
    "                match=models.MatchValue(value=thread_id),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    documents = recall_memories.similarity_search(\n",
    "        query,\n",
    "        k=3,\n",
    "        filter=qdrant_filter,  # structured filter required by QdrantVectorStore\n",
    "    )\n",
    "\n",
    "    return [doc.page_content for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d5ff780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from  backend\n",
    "user_id = \"user_123\"\n",
    "thread_id = \"thread_456\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"user_id\": user_id,\n",
    "        \"thread_id\": thread_id\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "926f1ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='погода' metadata={'user_id': 'user_123', 'thread_id': 'thread_456'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'погода'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_recall_memories(\"погода\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67c3768d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['погода']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"погода\"\n",
    "\n",
    "search_recall_memories(query,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5da83518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('user_123', 'thread_456')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_thread_id(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca89dab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vectors shape: 2 x 768\n"
     ]
    }
   ],
   "source": [
    "texts = [\"тест\", \"пример текста\"]\n",
    "vectors = embeddings.embed_documents(texts)\n",
    "print(\"Embedding vectors shape:\", len(vectors), \"x\", len(vectors[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a932b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Record(id='a2767bf6-eb6e-4a6a-a23b-5da19f352575', payload={'page_content': 'погода', 'metadata': {'user_id': 'user_123', 'thread_id': 'thread_456'}}, vector=None, shard_key=None, order_value=None)],\n",
       " None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = 'recall_memories'\n",
    "\n",
    "\n",
    "def search_recall_memories(\n",
    "        query: str,\n",
    "        config: RunnableConfig\n",
    ") -> List[str]:\n",
    "    \"\"\"Search for relevant recall memories.\"\"\"\n",
    "\n",
    "    user_id, thread_id = get_user_thread_id(config)\n",
    "    \n",
    "    # Filter by user_id and thread_id\n",
    "    qdrant_filter = models.Filter(\n",
    "        must=[\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.user_id\",\n",
    "                match=models.MatchValue(value=user_id),\n",
    "            ),\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.thread_id\",\n",
    "                match=models.MatchValue(value=thread_id),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    documents = recall_memories.similarity_search(\n",
    "        query,\n",
    "        k=3,\n",
    "        filter=qdrant_filter,  # structured filter required by QdrantVectorStore\n",
    "    )\n",
    "\n",
    "    return [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae10ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
