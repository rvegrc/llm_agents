{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea95574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import  END, START, MessagesState, StateGraph\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from IPython.display import Image, display\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Import necessary tools\n",
    "from tools.memory import save_memories, search_memories\n",
    "from tools.rag import vectorstore_collection_init, vectorstore_add_documents\n",
    "from tools.llm import llm_chat_tool\n",
    "from tools.search import search_tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LangSmith project\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = 'tg-bot'\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client_qd = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    messages: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15da93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model_name = '../../../models/multilingual-e5-large-instruct'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=emb_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_memories = vectorstore_collection_init(\n",
    "    client_qd=client_qd,\n",
    "    collection_name='recall_memories',\n",
    "    embeddings=embeddings,\n",
    "    distance=\"Cosine\"\n",
    ")\n",
    "\n",
    "long_term_memory = vectorstore_collection_init(\n",
    "    client_qd=client_qd,\n",
    "    collection_name='long_term_memory',\n",
    "    embeddings=embeddings,\n",
    "    distance=\"Cosine\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"\"\"\n",
    "            'system'\n",
    "            You are a helpful assistant with advanced long-term memory capabilities. \n",
    "            Powered by a stateless LLM, you must rely on external tools and memory systems \n",
    "            to store information between conversations. You can also perform Retrieval-Augmented \n",
    "            Generation (RAG) to access relevant knowledge in real-time.\n",
    "\n",
    "         \n",
    "            ## MEMORY USAGE GUIDELINES\n",
    "            \n",
    "            1. Actively use memory tools (save_core_memory, save_recall_memory) to build a \n",
    "            comprehensive understanding of the user.\n",
    "            2. Make informed suppositions and extrapolations based on stored memories.\n",
    "            3. Regularly reflect on past interactions to identify patterns and preferences.\n",
    "            4. Update your mental model of the user with each new piece of information.\n",
    "            5. Cross-reference new information with existing memories for consistency.\n",
    "            6. Store emotional context and personal values alongside factual information.\n",
    "            7. Use memory to anticipate needs and tailor responses to the userâ€™s style.\n",
    "            8. Recognize and acknowledge changes in the user's situation or perspective.\n",
    "            9. Leverage memories to provide personalized examples and analogies.\n",
    "            10. Recall past challenges or successes to inform current problem-solving.\n",
    "\n",
    "            \n",
    "            ## RAG USAGE GUIDELINES\n",
    "            \n",
    "            - Use RAG every time you do internet search to get some external information.\n",
    "            - Use RAG when you need up-to-date, domain-specific, or context-specific information\n",
    "            - Use RAG to retrieve relevant documents or data that can enhance the conversation.\n",
    "            - Use RAG to provide accurate and timely responses to user queries.\n",
    "            - Use RAG to access a wide range of user conversation history.\n",
    "\n",
    "            \n",
    "            ## RECALL MEMORIES\n",
    "            \n",
    "            Recall memories are contextually retrieved based on the current conversation:\n",
    "            {recall_memories}\n",
    "\n",
    "            ## INSTRUCTIONS\n",
    "           \n",
    "            Engage with the user naturally, as a trusted colleague or friend. \n",
    "            Do not explicitly mention your memory or retrieval capabilities. \n",
    "            Instead, seamlessly integrate them into your responses. \n",
    "            Be attentive to subtle cues and underlying emotions. \n",
    "            Adapt your communication style to match the user's preferences and current emotional state. \n",
    "            If you use tools, call them internally and respond only after the tool operation \n",
    "            completes successfully.\n",
    "        \"\"\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2eeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "\n",
    "    vectorstore_long_term_memory.simalarity_search\n",
    "\n",
    "    conv_str = get_buffer_string(state[\"messages\"][-3:]) # get all messages in the conversation or change to 2-3\n",
    "    # conv_str = tokenizer.decode(tokenizer.encode(conv_str)[-2048:]) # tokenize last messages and limit to 2048 tokens\n",
    "    recall_memories = search_memories.invoke(conv_str, config)\n",
    "    return {\n",
    "        \"messages\": recall_memories,\n",
    "    }\n",
    "\n",
    "\n",
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | llm_with_tools\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" + \"\\n\".join(state[\"messages\"]) + \"\\n</recall_memory>\"\n",
    "    )\n",
    "    prediction = bound.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"recall_memories\": recall_str,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    vectorstore_add_documents(\n",
    "        client_qd=client_qd,\n",
    "        collection_name=collection_name,\n",
    "        documents=[Document(page_content=prediction.content)],\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    \"\"\"Determine whether to use tools or end the conversation based on the last message.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"tools\", \"__end__\"]: The next step in the graph.\n",
    "    \"\"\"\n",
    "    msg = state[\"messages\"][-1]\n",
    "    if msg.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f94f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent():\n",
    "    builder = StateGraph()\n",
    "    builder = StateGraph(State)\n",
    "\n",
    "    builder.add_node(load_memories)\n",
    "    builder.add_node(agent)\n",
    "    builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    builder.add_edge(START, \"load_memories\")\n",
    "    builder.add_edge(\"load_memories\", \"agent\")\n",
    "    builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "    builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    memory = InMemorySaver()\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    \n",
    "\n",
    "    return builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"recall_memory\"\n",
    "\n",
    "vectorstore_recall = rag_tool(\n",
    "    client_qd=client_qd,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bee496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "# from langgraph_agent import build_agent\n",
    "\n",
    "app = FastAPI()\n",
    "agent = build_agent()\n",
    "\n",
    "class UserInput(BaseModel):\n",
    "    message: str\n",
    "    session_id: str  # Optional: for multi-user memory\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(user_input: UserInput):\n",
    "    state = {\"input\": user_input.message, \"session_id\": user_input.session_id}\n",
    "    result = agent.invoke(state)\n",
    "    return {\"response\": result.get(\"input\", \"Sorry, something went wrong.\")}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
